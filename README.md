# topsis-pretrained-102117089

## Overview
Selecting the most suitable pretrained model for a specific task using Topsis.

## Result:

`102117089-result.csv`

![Topsis Result](https://raw.githubusercontent.com/vteam27/topsis-pretrained-102117089/main/Topsis%20Result.png)

### Metrics Included:

1. **Accuracy:** This metric gauges the ratio of correct predictions to the total number of predictions, serving as a foundational measure for assessing both classification and regression models.

2. **Precision:** Precision quantifies the proportion of true positive predictions relative to the total number of positive predictions, indicating the model's adeptness at minimizing false positives.

3. **Recall:** Also known as sensitivity, recall measures the ratio of true positive predictions to the total number of actual positive instances, reflecting the model's capacity to detect all pertinent instances.

4. **F1 Score:** The F1 score, a blend of precision and recall through harmonic mean, offers a unified metric for evaluating models, particularly valuable in scenarios with class imbalances.

5. **BLEU (Bilingual Evaluation Understudy):** This metric assesses machine-translated text quality by comparing it with one or more reference translations, focusing on n-gram overlap between the candidate and reference translations.

6. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** ROUGE encompasses a suite of metrics for appraising the quality of summaries generated by automatic summarization systems, evaluating the concurrence between system-generated and reference summaries based on varied criteria.

7. **Perplexity:** Commonly applied to evaluate language model performance, perplexity gauges the model's predictive efficacy on a text sample, with lower values indicating higher confidence in predictions.

8. **Response Latency:** This metric quantifies the time taken by a system to generate a response or complete a task in response to input, crucial for real-time applications where swift responses are imperative for user satisfaction.

9. **Mean Opinion Score (MOS):** MOS serves as a subjective measure to evaluate the perceived quality of audio, video, or text generated by a system, typically derived from human ratings reflecting user perceptions of quality.


